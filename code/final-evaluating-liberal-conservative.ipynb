{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating How Liberal vs. Conservative Opinion News Shows Influenced the Narrative about the COVID-19 Pandemic\n",
    "\n",
    "**Group Members: Jennifer Andre, Tobi Jegede, Callie Lambert, & Lori Zakalik**\n",
    "\n",
    "**Date**: May 2, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** In order to run our team's notebook, please make sure you have the following python libraries and packages downloaded on your device:\n",
    "* glob\n",
    "* os\n",
    "* matplotlib.pyplot\n",
    "* numpy\n",
    "* string\n",
    "* regex\n",
    "* sklearn.feature_extraction\n",
    "* operator\n",
    "* collections\n",
    "* spacy\n",
    "* sklearn.decomposition\n",
    "* wordcloud\n",
    "* ntlk.sentiment\n",
    "    *  Note: If you run into issues after downloading the above package (ntlk.sentiment), please use the code chunk below: \n",
    "    \n",
    "        ```python\n",
    "        import nltk \n",
    "        import ssl\n",
    "        try: \n",
    "            _create_unverified_https_context = ssl._create_unverified_context\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            ssl._create_default_https_context = _create_unverified_https_context \n",
    "            \n",
    "        nltk.download('vader_lexicon')\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in our Final Project Proposal, we wanted to specifically analyze how the different **opinion** news arms of popular cable news channels talked about the COVID-19 pandemic. \n",
    "\n",
    "Using some research from the Pew Research Center, we found that CNN and MSNBC were the most popular news channels watched by individuals who consistently voted for liberal political candidates, while Fox News was the most watched cable news channel for individuals who consistently voted for conservaive political candidates. \n",
    "\n",
    "We then found additional articles that provided information on the most watched tv shows on CNN, MSNBC, and Fox News and found that Anderson Cooper 360 was the most watched show on CNN, Rachel Maddow was the most watched show on MSNBC, and Tucker Carlson and the Five were the most watched shows on Fox News. \n",
    "\n",
    "We then used webscraping techniques to pull the text files for the transcripts for each of the shows mentioned above from March 2020 to March 2022. The code to run this webscraping can be found in the code folder on our project's GitHub page, located here: https://github.com/tobijegede/opinion-news-nlp \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Data Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-780f9722bb2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "#import packages & libraries\n",
    "import glob \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import regex as re\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** You may need to adjust the code chunk below to properly load in the data. Our current file structure is:\n",
    "- Code Repository Folder (opinion-news-nlp)\n",
    "    - data\n",
    "        - 01-raw\n",
    "            - name of opinion news show\n",
    "                - a list of text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get the correct file path starting file path\n",
    "repo_path = os.path.dirname(os.getcwd()) \n",
    "\n",
    "#read in the liberal corpus\n",
    "rm_paths = glob.glob(repo_path + \"/data/01-raw/rachel_maddow/*.txt\") #the paths for the rachel maddow transcript files\n",
    "ac_paths = glob.glob(repo_path + \"/data/01-raw/anderson_cooper/*.txt\") #the paths for the anderson cooper transcript files\n",
    "all_liberal_files = rm_paths + ac_paths # all liberal trasncripts\n",
    "\n",
    "#read in the conservative corpus \n",
    "tc_paths = glob.glob(repo_path + \"/data/01-raw/tucker_carlson/*.txt\") #the paths for the tucker carlson transcript files\n",
    "tf_paths = glob.glob(repo_path + '/data/01-raw/the_five/*.txt') #the paths for the five transcript files\n",
    "all_conservative_files = tc_paths + tf_paths #all conservative transcripts\n",
    "\n",
    "#read in the CDC transcripts\n",
    "cdc_paths = glob.glob(repo_path + \"/data/01-raw/cdc_press_releases/*.txt\") #the paths for cdc transcript files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic summary statistics for all of the data that we used in our analysis is as follows:\n",
    "1. Conservative Corpus, N = 458\n",
    "    - Tucker Carlson, 208\n",
    "    - The Five, 250\n",
    "2. Liberal Corpus, N = 1,008\n",
    "    - Anderson Cooper, 530\n",
    "    - Rachel Maddow, 478\n",
    "3. CDC, N = 47\n",
    "\n",
    "\n",
    "It is important to note that there are fewer conservative transcripts in the corpus. Fox News does not have transcripts available for every show (e.g., Tucker Carlson airs every weeknight, but there are only transcripts available for 1-2 shows per week). Although this a limitation, the conservative corpus still provided a large amount of unstructured data to compare against the liberal corpus. <<*add info about script length*>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define COVID Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below, we created lists of words to use to help with co-occurrence analysis later on in our notebook. We specifically wanted to catalog most of the ways that COVID, mask, and vaccine words could potentially show up in the transcripts in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid terms\n",
    "#covid_terms = ['coronavirus', 'covid', 'covid-19', 'covid-', 'covid19', 'virus']\n",
    "\n",
    "covid_terms = ['coronavirus', 'covid', 'covid-19', 'covid-', \n",
    "                'covid19', 'virus', 'sars', 'sars-', 'sars-cov-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaccine terms\n",
    "#vaccine_terms = ['vaccine', 'vaccination', 'vaccinated', 'vaccinated', 'mrna', 'booster', 'vax', 'vaxx', 'vaxxed']\n",
    "\n",
    "vaccine_terms = ['vaccine', 'vaccination', 'vaccinated', 'mrna', 'booster', 'vax', 'vaxx', \n",
    "                'vaxxed', 'pfizer', 'moderna', 'johnson', 'j&j']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask terms\n",
    "#mask_terms = ['mask', 'masking']\n",
    "\n",
    "mask_terms = ['mask', 'masking', 'n95', 'kn95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other COVID-related terms (can choose to use or not)\n",
    "other_terms = ['china', 'wuhan', 'mandate', 'pandemic', 'epidemic', 'virus',\n",
    "                'distancing', 'spread', 'immunity', 'incubation', 'quarantine']\n",
    "\n",
    "all_covid_terms = covid_terms + other_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code chunk below, we add specific stop words for the conservative, liberal, and CDC news sources in order to remove named entities, like the names of the talk show hosts, as well as the name of the CDC, from the list of relevant words to count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add network, host names for conservative news corpus\n",
    "add_stop_words_conservative = ['tucker', 'carlson', 'fox', 'news', 'five', \n",
    "                'greg', 'gutfeld', 'dana', 'perino', 'jesse', 'watters', \n",
    "                'jeanine', 'pirro', 'geraldo', 'rivera', 'jessica', 'tarlov',\n",
    "                'harold', 'ford', 'jr', 'ok', 'williams',  'pavlich', 'fauci', 'faucis'\n",
    "                'mcdowell', 'juan', 'thanks', 'crosstalk', 'unidentified',\n",
    "                 'video', 'clip', 'voiceover', 'videotape']\n",
    "\n",
    "# add host names, important figures for the liberal news corpus\n",
    "add_stop_words_liberal = ['anderson', 'cooper', 'rachel', 'maddow', \n",
    "                  'chris', 'hayes', 'ari', 'berman', 'michael', 'osterholm',\n",
    "                  'cnn', 'msnbc', 'cnns', 'msnbcs',\n",
    "                  'vivek', 'murthy', 'rochelle', 'walensky', 'jerome', 'adams', 'alex', 'azar',\n",
    "                  'anthony', 'fauci', 'faucis',\n",
    "                  'cuomo', 'erin', 'david',\n",
    "                  'leana', 'wen', 'deborah', 'birx',\n",
    "                  'robert', 'redfield', 'gavin', 'newsom',\n",
    "                  'ashish', 'jha', 'tom', 'frieden',\n",
    "                  'video', 'clip', 'voiceover', 'videotape']\n",
    "\n",
    "# add common words for the cdc news corpus\n",
    "add_stop_words_cdc = ['question', 'cdc', \"fauci\", \"dr\", \"thanks\", \"thank\", \"people\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the full list of stop words for each of the corpuses\n",
    "full_stop_words_conservative = text.ENGLISH_STOP_WORDS.union(add_stop_words_conservative)\n",
    "full_stop_words_liberal = text.ENGLISH_STOP_WORDS.union(add_stop_words_liberal)\n",
    "full_stop_words_cdc = text.ENGLISH_STOP_WORDS.union(add_stop_words_cdc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in, Clean, & Store Cleaned Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import conservative corpus\n",
    "\n",
    "#get a list of file names to read into the dataset\n",
    "repo_path = os.path.dirname(os.getcwd())\n",
    "tc_paths = glob.glob(repo_path + \"/data/01-raw/tucker_carlson/*.txt\") #the paths for the tucker carlson transcript files\n",
    "tf_paths = glob.glob(repo_path + '/data/01-raw/the_five/*.txt') #the paths for the five transcript files\n",
    "\n",
    "all_conservative_files = tc_paths + tf_paths #all conservative transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import liberal corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CDC corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Transcripts by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis #1: Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a high level picture of the datasets that we had available, and to do a basic sense check, we first conducted word frequency analysis using sklearn's Count Vectorizer and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis #2: Co-Occurrence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis #3: Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis #4: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSERT SOMETHING HERE ABOUT POLICY IMPLICATIONS**\n",
    "1. LALALA\n",
    "2. OOHH LOOK AT MEEEEE, I'M POLICY RELATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work & Analysis Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future Work**\n",
    "1. Look at transcripts of **local news channels** instead of national news channels to get a better approximation of localized opinions about COVID-19 and its associated mitigation strategies\n",
    "    - This matches the way that COVID-19 mitigation is being addressed now â€“ on a case by case, local level\n",
    "2. Expand to look at more **change over time** in the coverage of COVID-19 under different presidential administrations instead of lumping them together to see how the discussion of the pandemic has changed over time\n",
    "\n",
    "**Limitation**\n",
    "1. By just doing text analysis, the **context** and **tone** surrounding the words is absent\n",
    "    - We found that the words used across the liberal and conservative news channels is the same but the context in which its being used or the tone with which the words are used could be different (and meaningfully so) but this is unable to be captured through only text analysis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
