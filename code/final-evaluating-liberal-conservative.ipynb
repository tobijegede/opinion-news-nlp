{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating How Liberal vs. Conservative Opinion News Shows Influenced the Narrative about the COVID-19 Pandemic\n",
    "\n",
    "**Group Members: Jennifer Andre, Tobi Jegede, Callie Lambert, & Lori Zakalik**\n",
    "\n",
    "**Date**: May 2, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** In order to run our team's notebook, please make sure you have the following python libraries and packages downloaded on your device:\n",
    "* glob\n",
    "* os\n",
    "* matplotlib.pyplot\n",
    "* numpy\n",
    "* string\n",
    "* regex\n",
    "* sklearn.feature_extraction\n",
    "* operator\n",
    "* collections\n",
    "* spacy\n",
    "* sklearn.decomposition\n",
    "* wordcloud\n",
    "* ntlk.sentiment\n",
    "    *  Note: If you run into issues after downloading the above package (ntlk.sentiment), please use the code chunk below: \n",
    "    \n",
    "        ```python\n",
    "        import nltk \n",
    "        import ssl\n",
    "        try: \n",
    "            _create_unverified_https_context = ssl._create_unverified_context\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            ssl._create_default_https_context = _create_unverified_https_context \n",
    "            \n",
    "        nltk.download('vader_lexicon')\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in our Final Project Proposal, we wanted to specifically analyze how the different **opinion** news arms of popular cable news channels talked about the COVID-19 pandemic. \n",
    "\n",
    "Using some research from the Pew Research Center, we found that CNN and MSNBC were the most popular news channels watched by individuals who consistently voted for liberal political candidates, while Fox News was the most watched cable news channel for individuals who consistently voted for conservaive political candidates. \n",
    "\n",
    "We then found additional articles that provided information on the most watched tv shows on CNN, MSNBC, and Fox News and found that Anderson Cooper 360 was the most watched show on CNN, Rachel Maddow was the most watched show on MSNBC, and Tucker Carlson and the Five were the most watched shows on Fox News. \n",
    "\n",
    "We then used webscraping techniques to pull the text files for the transcripts for each of the shows mentioned above from March 2020 to March 2022. The code to run this webscraping can be found in the code folder on our project's GitHub page, located here: https://github.com/tobijegede/opinion-news-nlp \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Data Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages & libraries\n",
    "import glob \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import regex as re\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** You may need to adjust the code chunk below to properly load in the data. Our current file structure is:\n",
    "- Code Repository Folder (opinion-news-nlp)\n",
    "    - data\n",
    "        - 01-raw\n",
    "            - name of opinion news show\n",
    "                - a list of text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get the correct file path starting file path\n",
    "repo_path = os.path.dirname(os.getcwd()) \n",
    "\n",
    "#read in the liberal corpus\n",
    "rm_paths = glob.glob(repo_path + \"/data/01-raw/rachel_maddow/*.txt\") #the paths for the rachel maddow transcript files\n",
    "ac_paths = glob.glob(repo_path + \"/data/01-raw/anderson_cooper/*.txt\") #the paths for the anderson cooper transcript files\n",
    "all_liberal_files = rm_paths + ac_paths # all liberal trasncripts\n",
    "\n",
    "#read in the conservative corpus \n",
    "tc_paths = glob.glob(repo_path + \"/data/01-raw/tucker_carlson/*.txt\") #the paths for the tucker carlson transcript files\n",
    "tf_paths = glob.glob(repo_path + '/data/01-raw/the_five/*.txt') #the paths for the five transcript files\n",
    "all_conservative_files = tc_paths + tf_paths #all conservative transcripts\n",
    "\n",
    "#read in the CDC transcripts\n",
    "cdc_paths = glob.glob(repo_path + \"/data/01-raw/cdc_press_releases/*.txt\") #the paths for cdc transcript files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic summary statistics for all of the data that we used in our analysis is as follows:\n",
    "1. Conservative Corpus, N = 458\n",
    "    - Tucker Carlson, 208\n",
    "    - The Five, 250\n",
    "2. Liberal Corpus, N = 1,008\n",
    "    - Anderson Cooper, 530\n",
    "    - Rachel Maddow, 478\n",
    "3. CDC, N = 47\n",
    "\n",
    "\n",
    "It is important to note that there are fewer conservative transcripts in the corpus. Fox News does not have transcripts available for every show (e.g., Tucker Carlson airs every weeknight, but there are only transcripts available for 1-2 shows per week). Although this a limitation, the conservative corpus still provided a large amount of unstructured data to compare against the liberal corpus. <<*add info about script length*>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define COVID Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below, we created lists of words to use to help with co-occurrence analysis later on in our notebook. We specifically wanted to catalog most of the ways that COVID, mask, and vaccine words could potentially show up in the transcripts in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid terms\n",
    "#covid_terms = ['coronavirus', 'covid', 'covid-19', 'covid-', 'covid19', 'virus']\n",
    "\n",
    "covid_terms = ['coronavirus', 'covid', 'covid-19', 'covid-', \n",
    "                'covid19', 'virus', 'sars', 'sars-', 'sars-cov-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaccine terms\n",
    "#vaccine_terms = ['vaccine', 'vaccination', 'vaccinated', 'vaccinated', 'mrna', 'booster', 'vax', 'vaxx', 'vaxxed']\n",
    "\n",
    "vaccine_terms = ['vaccine', 'vaccination', 'vaccinated', 'mrna', 'booster', 'vax', 'vaxx', \n",
    "                'vaxxed', 'pfizer', 'moderna', 'johnson', 'j&j']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask terms\n",
    "#mask_terms = ['mask', 'masking']\n",
    "\n",
    "mask_terms = ['mask', 'masking', 'n95', 'kn95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other COVID-related terms (can choose to use or not)\n",
    "other_terms = ['china', 'wuhan', 'mandate', 'pandemic', 'epidemic', 'virus',\n",
    "                'distancing', 'spread', 'immunity', 'incubation', 'quarantine']\n",
    "\n",
    "all_covid_terms = covid_terms + other_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code chunk below, we add specific stop words for the conservative, liberal, and CDC news sources in order to remove named entities, like the names of the talk show hosts, as well as the name of the CDC, from the list of relevant words to count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add network, host names for conservative news corpus\n",
    "add_stop_words_conservative = ['tucker', 'carlson', 'fox', 'news', 'five', \n",
    "                'greg', 'gutfeld', 'dana', 'perino', 'jesse', 'watters', \n",
    "                'jeanine', 'pirro', 'geraldo', 'rivera', 'jessica', 'tarlov',\n",
    "                'harold', 'ford', 'jr', 'ok', 'williams',  'pavlich', 'fauci', 'faucis'\n",
    "                'mcdowell', 'juan', 'thanks', 'crosstalk', 'unidentified',\n",
    "                 'video', 'clip', 'voiceover', 'videotape']\n",
    "\n",
    "# add host names, important figures for the liberal news corpus\n",
    "add_stop_words_liberal = ['anderson', 'cooper', 'rachel', 'maddow', \n",
    "                  'chris', 'hayes', 'ari', 'berman', 'michael', 'osterholm',\n",
    "                  'cnn', 'msnbc', 'cnns', 'msnbcs',\n",
    "                  'vivek', 'murthy', 'rochelle', 'walensky', 'jerome', 'adams', 'alex', 'azar',\n",
    "                  'anthony', 'fauci', 'faucis',\n",
    "                  'cuomo', 'erin', 'david',\n",
    "                  'leana', 'wen', 'deborah', 'birx',\n",
    "                  'robert', 'redfield', 'gavin', 'newsom',\n",
    "                  'ashish', 'jha', 'tom', 'frieden',\n",
    "                  'video', 'clip', 'voiceover', 'videotape']\n",
    "\n",
    "# add common words for the cdc news corpus\n",
    "add_stop_words_cdc = ['question', 'cdc', \"fauci\", \"dr\", \"thanks\", \"thank\", \"people\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the full list of stop words for each of the corpuses\n",
    "full_stop_words_conservative = text.ENGLISH_STOP_WORDS.union(add_stop_words_conservative)\n",
    "full_stop_words_liberal = text.ENGLISH_STOP_WORDS.union(add_stop_words_liberal)\n",
    "full_stop_words_cdc = text.ENGLISH_STOP_WORDS.union(add_stop_words_cdc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in, Clean, & Store Cleaned Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- IMPORTING & CLEANING THE LIBERAL CORPUS\n",
    "text_transcripts_liberal = [] # container for transcript text\n",
    "text_sentences_liberal = [] # container for sentence text: NOTE: WILL USE TEXT_SENTENCES FOR THE SENTIMENT ANALYSIS\n",
    "covid_counts_transcripts_liberal = [] # container for transcript covid mention counts\n",
    "covid_counts_sentences_liberal  = [] # container for sentence covid mention counts\n",
    "\n",
    "# read in each transcript, do text cleaning, add text and counts to containers\n",
    "# split sentences on periods, do text cleaning, add text and counts to containers\n",
    "# text cleaning is done separately for transcripts and sentences to help split sentences better\n",
    "\n",
    "#### transcripts as documents\n",
    "for transcript in all_liberal_files:\n",
    "    \n",
    "    # create transcript covid counter\n",
    "    cov_trans_counter = 0\n",
    "    \n",
    "    # read text\n",
    "    text = open(transcript, encoding = 'utf-8').read().lower()\n",
    "    \n",
    "    ### string cleaning\n",
    "    # line below replaces periods with spaces because spaces after periods are often missing, then strips extra spaces\n",
    "    corpus_text = text.replace('\\xa0', '').replace('.', ' ').replace('  ', ' ')\n",
    "    # remove all punctuation\n",
    "    for c in string.punctuation:\n",
    "        corpus_text = corpus_text.replace(c, \"\")\n",
    "    # remove numbers\n",
    "    corpus_text = re.sub('\\d+', '', corpus_text)\n",
    "    # remove whitespaces\n",
    "    corpus_text = \" \".join(corpus_text.split())\n",
    "    \n",
    "    # add cleaned text to container\n",
    "    text_transcripts_liberal.append(corpus_text)\n",
    "    \n",
    "    # add to counter if word in text is in covid terms, add count to container\n",
    "    for word in corpus_text.split(' '):\n",
    "        if word in covid_terms:\n",
    "            cov_trans_counter += 1\n",
    "    covid_counts_transcripts_liberal.append(cov_trans_counter)\n",
    "    \n",
    "    \n",
    "    #### sentences as documents\n",
    "    for sentence in text.split('.'): # note that splitting on periods is not perfect for identifying sentences\n",
    "        \n",
    "        # create sentence covid counter\n",
    "        cov_sen_counter = 0\n",
    "        \n",
    "        ### string cleaning\n",
    "        corpus_sen = sentence.replace('\\xa0', '')\n",
    "        # remove all punctuation    \n",
    "        for c in string.punctuation:\n",
    "            corpus_sen = corpus_sen.replace(c, \"\")\n",
    "        # remove numbers            \n",
    "        corpus_sen = re.sub('\\d+', '', corpus_sen)\n",
    "        # remove whitespaces\n",
    "        corpus_sen = \" \".join(corpus_sen.split())\n",
    "        \n",
    "        # add cleaned sentence to container\n",
    "        text_sentences_liberal.append(corpus_sen)\n",
    "        \n",
    "        # add to counter if word in sentence is in covid terms, add count to container\n",
    "        for word in corpus_sen.split(' '):\n",
    "            if word in covid_terms:\n",
    "                cov_sen_counter += 1\n",
    "        covid_counts_sentences_liberal.append(cov_sen_counter)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CDC corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Transcripts by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis #1: Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a high level picture of the datasets that we had available, and to do a basic sense check, we first conducted word frequency analysis using sklearn's Count Vectorizer and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis #2: Co-Occurrence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis #3: Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis #4: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key reason why we chose to do sentiment analysis as part of our project was because we theorized that, while conservative and liberal opinion tlk shows hosts and their guests may use the same words, the tone and sentiment around those words could drastically differ. \n",
    "\n",
    "**Background:** For the sentiment analysis portion of our analysis, we used a method called VADER (Valence Aware Dictionary and sEntiment Reasoner). The associated python package for this method is **nltk.sentiment**. This method works best on small pieces of data, like sentences, rather than on a longer trasncript. Therefore, the analysis below is organized at the **sentence-level**. Also, because we used a pre-exisiting package, the methodology for assigning \"positive\" or \"negative\" sentiments to the sentiments is a bit of a black box to us. If we had further time for this project, we would explore other sentiment analysis packages and compare the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Subset the List of Sentences That Include COVID Terms \n",
    "\n",
    "The goal here is to better understand the context/sentiment around how covid is discussed in the liberal, conservative, and cdc corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13,752 (3.0% of the total) sentences across the corpus that mention one of the covid-related terms.\n"
     ]
    }
   ],
   "source": [
    "covid_sentences_liberal = [sentence for sentence in text_sentences_liberal if any(word in sentence for word in covid_terms)]\n",
    "print(\"There are {:,} ({:.1%} of the total) sentences across the corpus that mention one of the covid-related terms.\".format(len(covid_sentences), len(covid_sentences)/len(text_sentences_liberal)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Apply the Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSERT SOMETHING HERE ABOUT POLICY IMPLICATIONS**\n",
    "1. LALALA\n",
    "2. OOHH LOOK AT MEEEEE, I'M POLICY RELATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work & Analysis Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future Work**\n",
    "1. Look at transcripts of **local news channels** instead of national news channels to get a better approximation of localized opinions about COVID-19 and its associated mitigation strategies\n",
    "    - This matches the way that COVID-19 mitigation is being addressed now – on a case by case, local level\n",
    "2. Expand to look at more **change over time** in the coverage of COVID-19 under different presidential administrations instead of lumping them together to see how the discussion of the pandemic has changed over time\n",
    "\n",
    "**Limitation**\n",
    "1. By just doing text analysis, the **context** and **tone** surrounding the words is absent\n",
    "    - We found that the words used across the liberal and conservative news channels is the same but the context in which its being used or the tone with which the words are used could be different (and meaningfully so) but this is unable to be captured through only text analysis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
