{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cdc_newsroom_url = \"https://www.cdc.gov/media/releases/2020/archives.html\"\n",
    "\n",
    "#the specific cdc feed on COVID-19 \n",
    "cdc_newsroom_url = \"https://www.cdc.gov/media/dpk/diseases-and-conditions/coronavirus/coronavirus-2020.html\"\n",
    "\n",
    "#step 2: get to the landing page of the website\n",
    "landing_page = requests.get(cdc_newsroom_url)\n",
    "\n",
    "#step 3: convert page into html\n",
    "soup = BeautifulSoup(landing_page.content, \"html.parser\")\n",
    "\n",
    "# <div class=\"tab-pane active\" role=\"tabpanel\" id=\"tabs-1-3\">\n",
    "#<div class=\"pressrel\">\n",
    "press_release_feed = soup.find_all(\"div\", class_= \"pressrel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(press_release_feed[:1])\n",
    "\n",
    "# \"item-title\" (this has a link to the page)\n",
    "\n",
    "# \"item-pubdate\" \n",
    "#<a class=\"item-title\" href=\"/media/releases/2021/p1203-covid-testing-tightens-intl.html\">CDC Tightens Testing Requirement for International Travel to the US to One Day</a> - <span class=\"item-pubdate\">Friday, December 3, 2021</span>\n",
    "print(press_release_feed.prettify()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f93f6cd32814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# <span class=\"item-pubdate\">\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[0mpub_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"span\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"item-pubdate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m    \u001b[0mtranscript_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstarting_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"item-title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"href\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m    \u001b[0mtranscript_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"item-title\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[\"href\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    " #step 5: get all of the transcript cards -- creates an object that is iterable to look at\n",
    "\n",
    "transcript_info = {} #key = air date, value is a dictionary with the (transcript url, guest, and text of the show)\n",
    "starting_url = \"https://www.cdc.gov\"\n",
    "for pr in press_release_feed: # data seems to be coming from both the transcripts and press releases tabs on the CDC)\n",
    "   # <span class=\"item-pubdate\">\n",
    "    pub_dates = pr.find_all(\"span\", class_ = \"item-pubdate\")\n",
    "#     transcript_url = starting_url + pr.find_all(\"a\", class_=\"item-title\")[\"href\"] \n",
    "#     transcript_url = pr.find_all(\"a\", class_=\"item-title\") #[\"href\"]\n",
    "#     print(transcript_url)\n",
    "#     print(pub_date) #, transcript_url)\n",
    "\n",
    "   # <a class=\"item-title\" href=\"/media/releases/2021/p1203-covid-testing-tightens-intl.html\">CDC Tightens Testing Requirement for International Travel to the US to One Day</a> - <span class=\"item-pubdate\">Friday, December 3, 2021</span>\n",
    "\n",
    "\n",
    "\n",
    "# for card in transcript_cards: #all information is stored at the date level\n",
    "#     air_date = card.find(\"div\", class_=\"transcript-card__air-date\").text\n",
    "#     transcript_url = card.find(\"a\", class_=\"transcript-card__show-name\")[\"href\"] #get the link for the website\n",
    "#     guests = card.find(\"span\", class_=\"transcript-card__guests\").text[8:].split(\",\")#removes a \"guests:\" tag from in front of the list of guests \n",
    "\n",
    "#     transcript_info[air_date] = {\"transcript_url\": transcript_url,\n",
    "#                                 \"guests\": guests }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function \n",
    "\n",
    "def get_transcript(search_url, folder_name):\n",
    "    '''\n",
    "    Input: search_url (string): this is the link to the SEARCH LANDING PAGE (i.e. NOT the actually site that has the transcript text)\n",
    "           folder_name (string)\n",
    "    Output: None (does not return anything but writes the text of the transript to a file)\n",
    "\n",
    "    '''\n",
    "\n",
    "    # ----------  GET TO THE WEBISTE & EXTRACT HTML\n",
    "    #step 1: store the URL\n",
    "    # \"https://www.msnbc.com/transcripts/show/rachel-maddow-show?dateRange=2020-03-01+TO+2022-03-31\"\n",
    "    rachel_maddow_URL = search_url\n",
    "\n",
    "    #step 2: get to the landing page of the website\n",
    "    landing_page = requests.get(rachel_maddow_URL)\n",
    "\n",
    "    #step 3: convert page into html\n",
    "    soup = BeautifulSoup(landing_page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "    #------ PARSE THROUGH THE HTML\n",
    "    #step 4: extract the transcript feed page part of the soup\n",
    "    transcript_feed = soup.find(class_=\"transcripts-feed\")\n",
    "\n",
    "    #step 5: get all of the transcript cards -- creates an object that is iterable to look at\n",
    "    transcript_cards = transcript_feed.find_all(\"div\", class_ =\"transcript-card\")\n",
    "\n",
    "    #step 6: terate through the all of the transcript cards on the page & extract the relevant information\n",
    "    transcript_info = {} #key = air date, value is a dictionary with the (transcript url, guest, and text of the show)\n",
    "\n",
    "    for card in transcript_cards: #all information is stored at the date level\n",
    "        air_date = card.find(\"div\", class_=\"transcript-card__air-date\").text\n",
    "        transcript_url = card.find(\"a\", class_=\"transcript-card__show-name\")[\"href\"] #get the link for the website\n",
    "        guests = card.find(\"span\", class_=\"transcript-card__guests\").text[8:].split(\",\")#removes a \"guests:\" tag from in front of the list of guests \n",
    "\n",
    "        transcript_info[air_date] = {\"transcript_url\": transcript_url,\n",
    "                                    \"guests\": guests }\n",
    "\n",
    "    # --- GET THE ACTUAL TRANSCRIPT TEXT\n",
    "\n",
    "    #step 7: iterate through all of the air dates to get the actual text of the transcript + write the text to a file \n",
    "    for air_date in list(transcript_info.keys()):\n",
    "\n",
    "        #get needed information\n",
    "        show_url = transcript_info[air_date][\"transcript_url\"]\n",
    "        #print(show_url)\n",
    "        \n",
    "        #go to the url\n",
    "        show_page = requests.get(show_url)\n",
    "\n",
    "        #convert the page into a soup object\n",
    "        show_soup =  BeautifulSoup(show_page.content, \"html.parser\")\n",
    "\n",
    "        #get the content of the transcript = <div class=\"article-body__content\">\n",
    "\n",
    "        #extract the transcript feed page part of the soup\n",
    "        transcript_text = show_soup.find(class_=\"article-body__content\").text\n",
    "\n",
    "        #add the transcript text to the dictionary\n",
    "        transcript_info[air_date][\"transcript_text\"] = transcript_text\n",
    "\n",
    "    # --- NOW THAT WE HAVE THE ACTUAL TEXT OF THE TRANSCRIPT, EXPORT THIS TO A TEXT FILE\n",
    "        \n",
    "        #create a file name + output file path\n",
    "        list_air_date = air_date.split()\n",
    "\n",
    "\n",
    "        filename = list_air_date[0] + \"_\" + list_air_date[1] + \"_\" + list_air_date[2] + \"_\" + \"rachel\" + \"_\" + \"maddow\" + \"_\" + \"transcript.txt\"\n",
    "        repo_path = os.path.dirname(os.getcwd())\n",
    "        output_path = os.path.join(repo_path, \"data\", \"01-raw\", folder_name, filename)\n",
    "\n",
    "        #print(output_path)\n",
    "\n",
    "        #export the file\n",
    "        with open(output_path,\"w\") as f:\n",
    "            f.write(transcript_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-- there are 25 pages worth of data in the specified date range\n",
    "\n",
    "Example url (WITH SPECIFIED DATE RANGE): https://www.msnbc.com/transcripts/show/rachel-maddow-show?page=2&dateRange=2020-03-01+TO+2022-03-31 \n",
    "\n",
    "starting URL: https://www.msnbc.com/transcripts/show/rachel-maddow-show?dateRange=2020-03-01+TO+2022-03-31 \n",
    "\n",
    "all other pages:\n",
    "https://www.msnbc.com/transcripts/show/rachel-maddow-show?page=2&dateRange=2020-03-01+TO+2022-03-31\n",
    "\n",
    "total_num_pages = 25\n",
    "\n",
    "for i in range(total_num_pages):\n",
    "    page_num = i + 1 \n",
    "    if page_num == 1:\n",
    "     URL = https://www.msnbc.com/transcripts/show/rachel-maddow-show?dateRange=2020-03-01+TO+2022-03-31 \n",
    "    \n",
    "    #insert function here\n",
    "\n",
    "    #iteratively export the files to the folder\n",
    "\n",
    "    else: \n",
    "    URL = \"https://www.msnbc.com/transcripts/show/rachel-maddow-show?page=\"+ page_num +  \"&dateRange=2020-03-01+TO+2022-03-31\"\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "total_num_pages = 25\n",
    "\n",
    "folder_name = \"cdc_press_releases\"\n",
    "\n",
    "#note: There is an if statement bc the URL is different for the first page\n",
    "for i in range(total_num_pages):\n",
    "    page_num = i + 1 \n",
    "\n",
    "    if page_num == 1:\n",
    "        URL = \"https://www.msnbc.com/transcripts/show/rachel-maddow-show?dateRange=2020-03-01+TO+2022-03-31\"\n",
    "\n",
    "        #call transcripts function\n",
    "        get_transcript(URL, folder_name)\n",
    "\n",
    "\n",
    "    else: \n",
    "        URL = \"https://www.msnbc.com/transcripts/show/rachel-maddow-show?page=\"+ str(page_num) +  \"&dateRange=2020-03-01+TO+2022-03-31\"\n",
    "\n",
    "        #call transcripts function\n",
    "        get_transcript(URL, folder_name)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
